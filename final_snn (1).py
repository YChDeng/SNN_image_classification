# -*- coding: utf-8 -*-
"""FInal_SNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19BjElcHalm4Tcp1rXChS3zAXcuxQwhgS

## Installing snntorch library
"""

!pip install snntorch --quiet
!pip install -q kaggle
!pip install torchview -q
!pip install torchshape -q

"""## Importing all the necessary tools"""

# imports
import numpy as np
import itertools
import cv2
from google.colab import files
import timeit
from os import listdir
import os
from zipfile import ZipFile
import gdown
import shutil

import snntorch as snn
from snntorch import spikeplot as splt
from snntorch import spikegen

import torchvision.transforms.functional as TF
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
from torchview import draw_graph
from torchvision.transforms import ToTensor
from torchshape import tensorshape

import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
from IPython.display import HTML
from sklearn.metrics import classification_report

"""## Loading data from web and arranging in the filesystem
[Link](https://www.kaggle.com/datasets/ruizgara/socofing) to the SOCOFing dataset
"""

# When you run this cell, you need to load the kaggle.json file, which is the API 
# for getting the data. It is important that you agree to the 
# SOCOFing dataset usage rules. 
files.upload()
!mkdir ~/.kaggle

!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets list
!kaggle datasets download -d ruizgara/socofing -q
!mkdir original_data
!unzip socofing.zip -d .

# Load fingerprints dataset from kaggle and sort it
# Sort data inside train and test folders on classes folders named from 0 to 9
map_finger_name = ['left_thumb', 'left_index', 'left_middle', 'left_ring', 'left_little',
           'right_thumb', 'right_index', 'right_middle', 'right_ring', 'right_little']

# Working directories
load_dir = 'SOCOFing/'

datadir = './datasets/'
finger_dir = datadir + 'fingerprints/'
finger_train_dir = finger_dir + 'train/'
finger_val_dir = finger_dir + 'val/'
finger_test_dir = finger_dir + 'test/'

if not os.path.exists(finger_dir):
    # Create folders
    os.mkdir('./datasets')
    os.mkdir(finger_dir)
    os.mkdir(finger_train_dir)
    os.mkdir(finger_val_dir)
    os.mkdir(finger_test_dir)

    for i in range(10):
        os.mkdir(finger_train_dir + f"{i}/")
        os.mkdir(finger_val_dir + f"{i}/")
        os.mkdir(finger_test_dir + f"{i}/")

    # Move and sort by classes files of train dataset
    # for dir in ['Real/', 'Altered/Altered-Easy/', 'Altered/Altered-Medium/', 'Altered/Altered-Hard/']:
    for dir in ['Real/', 'Altered/Altered-Easy/']:
        for img in os.listdir(load_dir + dir):
            ind = max([i if map_finger_name[i] in img.lower() else -1 for i in range(len(map_finger_name))])
            shutil.copy2(load_dir + dir + img, finger_train_dir + f"{ind}/" + img)

"""## Data preprocessing

"""

class CropInvertTransform:
    """
    Crop the frame (2*2*4*4) from the image, reformat to 1 channel, invert colors 
    and put black padding to the squared form. If the size is bigger, than the required, resize it
    """
    def __init__(self, size):
        self.size = size
        

    def __call__(self, x):
        """
        Transform the image by frame cropping, grayscaling and inverting
        Resize to the square by adding black padding

        :param x: tensor (image) to be trandformed
        :return: trandformed image
        """
        # crop рамку 2 2 4 4
        x = TF.crop(x, 2, 2, TF.get_image_size(x)[1] - 6, TF.get_image_size(x)[0] - 6)
        x = TF.rgb_to_grayscale(x)
        x = TF.invert(x)

        # to square
        a = max(TF.get_image_size(x)) - TF.get_image_size(x)[0]
        b = max(TF.get_image_size(x)) - TF.get_image_size(x)[1]

        x = TF.pad(x, [a // 2, b // 2, a - a // 2, b - b // 2], fill=0)
        if TF.get_image_size(x)[0] > self.size :
            x = TF.resize(x, [self.size, self.size], antialias=True)
        return x

fingerprints_transform = transforms.Compose([
    transforms.ToTensor(),
    CropInvertTransform(97),
    transforms.Normalize((0,), (1,))
])
# # Define a transform for emnist and fashion mnist
mnist_transform = transforms.Compose([
            transforms.Resize((28, 28)),
            transforms.Grayscale(),
            transforms.ToTensor(),
            transforms.Normalize((0,), (1,))])

# Show example before after transform
# Load the image
names = ['107__M_Left_index_finger.BMP', '107__M_Left_index_finger_Obl.BMP', '107__M_Left_index_finger_CR.BMP', '107__M_Left_index_finger_Zcut.BMP']

for i in range(len(names)):
    root='./datasets/fingerprints/'
    dirs = ['train/', 'val/','test/']
    for d in dirs:
        if names[i] in os.listdir(root+d+'1/'):
            break
    image = cv2.imread(root+d+'1/'+names[i])
    #Plot the original image
    plt.subplot(2, 4, i+1)
    # plt.title("Original")
    plt.imshow(image)

    #Plot the sharpened image
    plt.subplot(2, 4, i+5)
    
    arr = fingerprints_transform(image)[0].numpy()*255    
    plt.imshow(arr,cmap='gray')

plt.show()

"""### Transform - Datasets - Dataloaders"""

datadir = './datasets/'
finger_dir = datadir + 'fingerprints/'
finger_train_dir = finger_dir + 'train/'
emnist_dir = datadir + 'emnist/'
fashion_dir = datadir + 'fashion_mnist/'

train_dir = 'train/'
val_dir = 'val/'
test_dir = 'test/'

val_split = 0.1
test_split = 0.1

# dataloader arguments
batch_size = 128
dtype = torch.float
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

# Fingerprints

# validation test split
for finger_type in os.listdir(finger_train_dir):
    imgs = os.listdir(finger_train_dir + finger_type)
    np.random.shuffle(imgs)
    for img in imgs[:int(len(imgs) * test_split) + 1]:
        os.rename(finger_train_dir + finger_type + "/" + img, finger_dir + test_dir + finger_type + "/" + img)

    for img in imgs[int(len(imgs) * test_split) + 1:int(len(imgs) * test_split) + int(len(imgs) * val_split) + 2]:
        os.rename(finger_train_dir + finger_type + "/" + img, finger_dir + val_dir + finger_type + "/" + img)


# Set dataset
fingerprints_dataset = {'train': datasets.ImageFolder(root=finger_dir+train_dir, transform=fingerprints_transform),
                        'val': datasets.ImageFolder(root=finger_dir+val_dir, transform=fingerprints_transform),
                        'test': datasets.ImageFolder(root=finger_dir+test_dir, transform=fingerprints_transform)
}

# Set dataloader
fingerprints_dataloader = {'train': DataLoader(fingerprints_dataset['train'], batch_size=batch_size, shuffle=True, drop_last=True),
                        'val': DataLoader(fingerprints_dataset['val'], batch_size=batch_size, shuffle=True, drop_last=True),
                        'test': DataLoader(fingerprints_dataset['test'], batch_size=batch_size, shuffle=True, drop_last=True)
}

# EMNIST
# Set dataset
emnist_dataset = {'train': torchvision.datasets.EMNIST(emnist_dir+train_dir, 'digits', download=True, train=True, transform=mnist_transform),
                  'val': None,
                  'test': torchvision.datasets.EMNIST(emnist_dir+train_dir, 'digits',download=True, train=False, transform=mnist_transform)
}

# Validation split
emnist_dataset['train'], emnist_dataset['val'] = random_split(emnist_dataset['train'], [1-val_split, val_split])

# Set dataloaders
emnist_dataloader = {'train': DataLoader(emnist_dataset['train'], batch_size=batch_size, shuffle=True, drop_last=True),
                     'val': DataLoader(emnist_dataset['val'], batch_size=batch_size, shuffle=True, drop_last=True),
                     'test': DataLoader(emnist_dataset['test'], batch_size=batch_size, shuffle=True, drop_last=True)
}

# Fashion-MNIST
# Set dataset
fashion_dataset = {'train': torchvision.datasets.FashionMNIST(fashion_dir+train_dir, download=True, train=True, transform=mnist_transform),
                  'val': None,
                  'test': torchvision.datasets.FashionMNIST(fashion_dir+train_dir, download=True, train=False, transform=mnist_transform)
}

# Validation split
fashion_dataset['train'], fashion_dataset['val'] = random_split(fashion_dataset['train'], [1-val_split, val_split])

# Set dataloaders
fashion_dataloader = {'train': DataLoader(fashion_dataset['train'], batch_size=batch_size, shuffle=True, drop_last=True),
                     'val': DataLoader(fashion_dataset['val'], batch_size=batch_size, shuffle=True, drop_last=True),
                     'test': DataLoader(fashion_dataset['test'], batch_size=batch_size, shuffle=True, drop_last=True)
}

# print pictures from each dataset
dataloaders = [fingerprints_dataloader, emnist_dataloader, fashion_dataloader]
for i in range(3):
    pic, _ = next(iter(dataloaders[i]['train']))
    pic = pic[0][0].numpy()*255 

    plt.subplot(1, 3, i+1)
    plt.imshow(pic, cmap='gray')

plt.show()

"""## Animations and visualizations of dataset encoded as spike"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %matplotlib inline
# 
# img, label = next(iter(fingerprints_dataloader['train'])) 
# img = img[0][0] # first image in the batch of images (and first dimension)
# label = label[0].item()
# 
# # print(img[0].shape) # print shape of feature array in loader
# 
# num_steps = 100
# 
# encoded_img = spikegen.rate(img, num_steps=num_steps, gain=0.75)
# 
# print('Shape of encoded image', encoded_img.shape)
# 
# fig, ax = plt.subplots()
# image_plot = ax.imshow(img, cmap='gray')
# 
# # initialization function: plot the background of each frame
# def init():
#     image_plot.set_data(img)
#     ax.axis('off')
#     return image_plot
# 
# # animation function.  This is called sequentially
# def animate(time):
#     image_plot.set_array(encoded_img[time])
#     ax.set_title(f'Label: {label}. Time: {time}')
# 
# anim = FuncAnimation(fig, animate, init_func=init, frames=len(encoded_img), interval=100)

"""#### What does an snn see with the time"""

HTML(anim.to_html5_video())

"""#### Raster plot of diagonal pixels"""

# Reshape
raster_img = torch.Tensor([encoded_img[:,i,i].numpy() for i in range(len(encoded_img[0]))]).T

# raster plot
fig = plt.figure(facecolor="w", figsize=(10, 5))
ax = fig.add_subplot(111)
ax.set_title('Spiking of a diagonal pixels with the time')
ax.set_xlabel('Time')
ax.set_ylabel('Number of a diagonal pixel')
splt.raster(raster_img, ax, s=1, color='black')
None

"""#### Raster plot of a single diagonal pixel"""

def add_x_ticks(ax):
  ax.set_xticks(list(range(100))[::1])
  ax.tick_params(axis='x', which='major', labelsize=7)
  ax.set_yticks([])

def plot_spikes_with_potential(spikes, in_spikes, potential, title=None, threshold=1):
  fig,ax = plt.subplots(3,1, facecolor="w", figsize=(15, 5), gridspec_kw={'height_ratios': [0.5, 4, 0.5]})
  ax[2].autoscale(tight=True)
  ax[1].autoscale(tight=True)
  ax[0].autoscale(tight=True)

  # Spikes line
  ax[0].set_title(f"{int(spikes.sum())} spikes and potential in neuron" if title==None else title)
  ax[0].set_ylabel("Input spikes")
  splt.raster(in_spikes, ax[0], s=200, c="black", marker="|")
  add_x_ticks(ax[0])

  # Spikes line
  splt.raster(spikes, ax[2], s=200, c="black", marker="|")
  ax[2].set_xlabel("Time")
  ax[2].set_ylabel("Output spikes")
  add_x_ticks(ax[2])

  # Membrane potential graph
  ax[1].plot(potential)
  ax[1].set_ylabel("Membrane potential")
  add_x_ticks(ax[1])

  times = []
  for i,spike in enumerate(spikes):
    if (spike != 0):
      times.append(i)

  ax[1].vlines(x = times, ymin = potential.min(), ymax = potential.max(), colors = 'gray', ls='dashed', lw=0.5)
  # Threshold line
  ax[1].axhline(y = threshold, color = 'r', linestyle = '--')

  fig.tight_layout()

lif = snn.Lapicque(R=5.1, C=5e-3, time_step=1e-3, threshold=0.3)

delta_t = 1e-3
tau = 5.1*5e-3
b = np.exp(-delta_t/tau)
lif = snn.Leaky(beta = b, threshold=1.5)

spk_in = spikegen.rate_conv(torch.ones((99))*0.2)
mem = torch.ones(1)*0.1
spk = torch.zeros(1)
potential = [mem]
spikes = [spk]

# neuron simulation
for step in range(99):
  spk, mem = lif(spk_in[step], mem)
  potential.append(mem)
  spikes.append(spk)

# convert lists to tensors
potential = torch.stack(potential)
spikes = torch.stack(spikes)
plot_spikes_with_potential(spikes, spk_in, potential, title='Membrane potential of an output neuron and its spikes', threshold=1.5)

spikes = torch.Tensor(np.random.binomial(1,0.5, 100))
potential = torch.Tensor(np.random.binomial(10,0.5, 100))/5
# plot_spikes(raster_img[:, 51].unsqueeze(1))
plot_spikes_with_potential(spikes, potential, title='Membrane potential of an output neuron and its spikes')

"""### Visualizing output neurons' spikes with the time"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %matplotlib inline
# outputs = np.random.binomial(1, p=0.5, size=(100, 10))
# def animate_output(outputs, labels=None, figsize=(20,3), ax=None):
#   
#   if ax == None:
#     fig, ax = plt.subplots(figsize=figsize)
# 
#   if labels == None:
#     labels = list(range(outputs.shape[1]))
# 
#   positions = np.arange(len(labels))
# 
#   bars = ax.barh(positions, outputs[0], height=0.5,)
#   ax.set_yticks(positions)
#   ax.set_yticklabels(labels)
#   ax.set_xticks(list(range(outputs.shape[0]))[::2])
#   ax.set_xlim(0,outputs.shape[0])
#   ax.set_xlabel('Number of spikes')
#   ax.set_title('Spikes of each neuron')
# 
#   def update(time):
#     for i,bar in enumerate(bars):
#       bar.set_width(outputs[:time+1,i].sum())
# 
#     ax.set_title(f'Spikes of each neuron. Time: {time}')
# 
#   anim = FuncAnimation(fig, update, frames=100, interval=200)
# 
#   return anim
# 
# 
# anim = animate_output(outputs, labels=map_finger_name)

HTML(anim.to_html5_video())

"""# Model"""

def training(model, train_loader, val_loader, optimizer, criterion, 
             device, epochs, tolerance = 3, min_delta = 0.01, 
             snn_mode = False, num_steps = None):
    epochs = epochs
    num_steps = num_steps
    time_delta = 0
    total_val_history = []
    accuracy_history = []

    count = 0
    early_stop_val = np.inf
    
    start = timeit.default_timer()

    for epoch in range(epochs):
        model.train()

        train_loss, valid_loss = [], []
        total_train = 0
        correct_train = 0
        total_val = 0
        correct_val = 0

        for data, targets in train_loader:
            data = data.to(device)
            targets = targets.to(device)

            optimizer.zero_grad()
            
            if snn_mode:
                data = data.view(data.shape[0], -1)
                spike, potential = model(data)

                loss_val = torch.zeros((1), dtype=dtype, device=device)
                for step in range(num_steps):
                    loss_val += criterion(potential[step], targets)

                loss_val /= num_steps

                _, predicted = spike.sum(dim=0).max(1)

                total_train += targets.size(0)
                correct_train += (predicted == targets).sum().item()
            else:
                output = model(data)

                loss_val = criterion(output, targets)

                pred = output.argmax(dim=1, keepdim=True) 

                total_train += targets.size(0)
                correct_train += pred.eq(targets.view_as(pred)).sum().item()

            loss_val.backward()
            optimizer.step()
            train_loss.append(loss_val.item())

        with torch.no_grad():

            model.eval()

            for data, targets in val_loader:
                data = data.to(device)
                targets = targets.to(device)
                
                if snn_mode:
                    data = data.view(data.shape[0], -1)

                    spike, potential = model(data)

                    loss_val = torch.zeros((1), dtype=dtype, device=device)
                    for step in range(num_steps):
                        loss_val += criterion(potential[step], targets)

                    loss_val /= num_steps

                    _, predicted = spike.sum(dim=0).max(1)

                    total_val += targets.size(0)
                    correct_val += (predicted == targets).sum().item()

                else:
                    output = model(data)

                    loss_val = criterion(output, targets)
                    
                    pred = output.argmax(dim=1, keepdim=True) 

                    total_val += targets.size(0)
                    correct_val += pred.eq(targets.view_as(pred)).sum().item()

                valid_loss.append(loss_val.item())

            # Early stopping
            if abs(early_stop_val - np.mean(valid_loss)) < min_delta:
                count += 1
            else:
                count = 0
                   
            if count == tolerance:
                break

            early_stop_val = np.mean(train_loss)

            total_val_history.append(np.mean(valid_loss))
            accuracy_history.append(correct_val/total_val)
            print ("Epoch:", epoch, "\n\tTraining Loss:", np.mean(train_loss), 
                f"\n\tTraining Accuracy: {100 * correct_train/total_train:.2f}%", 
                "\n\tValidation Loss:", np.mean(valid_loss),
                f"\n\tValidation Accuracy: {100 * correct_val/total_val:.2f}%")
            
    stop = timeit.default_timer()
    time_delta = stop - start
    return total_val_history, accuracy_history, time_delta/(epoch + 1)


def testing(model, test_loader, device, snn_mode = False):
    total = 0
    correct = 0
    predictions = np.array([])
    true_labels = np.array([])

    with torch.no_grad():
        model.eval()
        for data, targets in test_loader:
            data = data.to(device)
            targets = targets.to(device)
            
            if snn_mode:
                test_spk, _ = model(data.view(data.size(0), -1))

                _, predicted = test_spk.sum(dim=0).max(1)
                predictions = np.concatenate((predictions, predicted.cpu()))
            

                total += targets.size(0)
                correct += (predicted == targets).sum().item()
            else:
                output = model(data)

                predicted = output.argmax(dim=1, keepdim=True) 
                predictions = np.concatenate((predictions, predicted.cpu().numpy().reshape(1,-1)[0]))
            
        
                total += targets.size(0)
                correct += predicted.eq(targets.view_as(predicted)).sum().item()

            true_labels = np.concatenate((true_labels, targets.cpu()))

    print(f"Total correctly classified test set images: {correct}/{total}")
    print(f"Test Set Accuracy: {100 * correct / total:.2f}%\n")
    return true_labels, predictions

class SNN(nn.Module):
    def __init__(self, num_inputs, num_outputs, 
                 num_hidden = 1024, num_steps = 25, 
                 beta = 0.95):
        super(SNN, self).__init__()

        # Initialize layers
        self.linear1 = nn.Linear(num_inputs, num_hidden)
        self.lif1 = snn.Leaky(beta=beta)
        self.linear2 = nn.Linear(num_hidden, num_outputs)
        self.lif2 = snn.Leaky(beta=beta)

        # Initialize number of time steps
        self.num_steps = num_steps

    def forward(self, x):

        # Initialize hidden states at t=0
        potential1 = self.lif1.init_leaky()
        potential2 = self.lif2.init_leaky()
        # mem3 = self.lif3.init_leaky()
        
        # Record the final layer
        output_spike = []
        output_potential = []

        for step in range(self.num_steps):
            # print(x.shape)
            current1 = self.linear1(x)
            spike1, potential1 = self.lif1(current1, potential1)
            current2 = self.linear2(spike1)
            spike2, potential2 = self.lif2(current2, potential2)
            
            output_spike.append(spike2)
            output_potential.append(potential2)
        return torch.stack(output_spike, dim=0), torch.stack(output_potential, dim=0)

class CNN(nn.Module):
    def __init__(self, inputs_shape=(128, 1,97,97), num_outputs=10, num_hidden = 1024):
        super(CNN, self).__init__()

        self.conv1 = nn.Sequential(
            nn.Conv2d(1,16,3),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )
        self.conv2 = nn.Sequential(
            nn.Conv2d(16,32,3),
            nn.ReLU(),
            nn.MaxPool2d(2)
        )
        outshape = [*tensorshape(self.conv2, tensorshape(self.conv1, inputs_shape))][1:]

        self.linear1 = nn.Sequential(
            nn.Linear(np.prod(outshape), num_hidden),
            nn.ReLU(),
            nn.Linear(num_hidden, num_outputs)
        )

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = torch.flatten(x, 1)
        x = self.linear1(x)
        return F.log_softmax(x, dim=1)

"""## Create models for different datasets

"""

max_epochs = 20
time_steps = 25
criterion = nn.CrossEntropyLoss

"""### Fingerprints

#### Models creation
"""

# Set parameters
finger_input_shape = next(iter(fingerprints_dataloader['train']))[0].shape
finger_num_inputs = np.prod(finger_input_shape[-2:])
finger_num_outputs = len(np.unique(fingerprints_dataloader['train'].dataset.targets))

dtype = torch.float
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

print(f'Device: {device}')

# Create SNN model
finger_snn = SNN(finger_num_inputs, finger_num_outputs).to(device)
finger_snn_optimizer = torch.optim.Adam(finger_snn.parameters(), lr=0.000085, betas=(0.9, 0.999))

print(finger_snn)

# Train SNN model
finger_snn_loss, finger_snn_accuracy, finger_snn_time = training(finger_snn, fingerprints_dataloader['train'], 
                                  fingerprints_dataloader['val'], finger_snn_optimizer, 
                                  criterion(), device, max_epochs, 
                                  snn_mode=True, num_steps=time_steps)

# Train SNN model
finger_snn_loss, finger_snn_accuracy, finger_snn_time = training(finger_snn, fingerprints_dataloader['train'], 
                                  fingerprints_dataloader['val'], finger_snn_optimizer, 
                                  criterion(), device, max_epochs, 
                                  snn_mode=True, num_steps=time_steps)

# Create CNN model
finger_cnn = CNN(finger_input_shape, finger_num_outputs).to(device)
finger_cnn_optimizer = torch.optim.Adam(finger_cnn.parameters(), lr=0.000085, betas=(0.9, 0.999))

print(finger_cnn)

# Train CNN model
finger_cnn_loss, finger_cnn_accuracy, finger_cnn_time = training(finger_cnn, fingerprints_dataloader['train'], 
                                  fingerprints_dataloader['val'], finger_cnn_optimizer, 
                                  criterion(), device, max_epochs, snn_mode=False)

"""#### Analisys of the results obtained by the models"""

print("Average time per epoch for SNN:", finger_snn_time, end=" sec")
print(classification_report(*testing(finger_snn, fingerprints_dataloader['test'], device, snn_mode = True)))

print("Average time per epoch for CNN:", finger_cnn_time, end=" sec")
print(classification_report(*testing(finger_cnn, fingerprints_dataloader['test'], device, snn_mode=False)))

# Plot validation loss and accuracy for both models
plt.figure(figsize=(15, 5))

plt.subplot(121)
plt.plot(finger_snn_loss)
plt.plot(finger_cnn_loss)

plt.ylabel('Validation loss')
plt.xlabel('Epochs')
plt.legend(['SNN', 'CNN'])

plt.subplot(122)
plt.plot(finger_snn_accuracy)
plt.plot(finger_cnn_accuracy)
plt.ylabel('Validation accuracy')
plt.xlabel('Epochs')
plt.legend(['SNN', 'CNN'])
plt.suptitle('Validation loss and accuracy for fingerprints classification')

plt.show()

"""### EMNIST 


"""

# Set parameters
emnist_input_shape = next(iter(emnist_dataloader['train']))[0].shape
emnist_num_inputs = np.prod(emnist_input_shape[-2:])
emnist_num_outputs = len(np.unique(emnist_dataloader['test'].dataset.targets))

dtype = torch.float
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

print(f'Device: {device}')

# Create SNN model
emnist_snn = SNN(emnist_num_inputs, emnist_num_outputs).to(device)
emnist_snn_optimizer = torch.optim.Adam(emnist_snn.parameters(), lr=0.000085, betas=(0.9, 0.999))

print(emnist_snn)

# Train SNN model
emnist_snn_loss, emnist_snn_accuracy, emnist_snn_time = training(emnist_snn, emnist_dataloader['train'], 
                                  emnist_dataloader['val'], emnist_snn_optimizer, 
                                  criterion(), device, max_epochs, 
                                  snn_mode=True, num_steps=time_steps)

# Create CNN model
emnist_cnn = CNN(emnist_input_shape, emnist_num_outputs).to(device)
emnist_cnn_optimizer = torch.optim.Adam(emnist_cnn.parameters(), lr=0.000085, betas=(0.9, 0.999))

print(emnist_cnn)

# Train CNN model
emnist_cnn_loss, emnist_cnn_accuracy, emnist_cnn_time = training(emnist_cnn, emnist_dataloader['train'], 
                                  emnist_dataloader['val'], emnist_cnn_optimizer, 
                                  criterion(), device, max_epochs, snn_mode=False)

"""#### Analisys of the results obtained by the models"""

print("Average time per epoch for SNN:", emnist_snn_time, end=" sec")
print(classification_report(*testing(emnist_snn, emnist_dataloader['test'], device, snn_mode = True)))

print("Average time per epoch for CNN:", emnist_cnn_time, end=" sec")
print(classification_report(*testing(emnist_cnn, emnist_dataloader['test'], device, snn_mode=False)))

# Plot validation loss and accuracy for both models
plt.figure(figsize=(15, 5))

plt.subplot(121)
plt.plot(emnist_snn_loss)
plt.plot(emnist_cnn_loss)

plt.ylabel('Validation loss')
plt.xlabel('Epochs')
plt.legend(['SNN', 'CNN'])

plt.subplot(122)
plt.plot(emnist_snn_accuracy)
plt.plot(emnist_cnn_accuracy)
plt.ylabel('Validation accuracy')
plt.xlabel('Epochs')
plt.legend(['SNN', 'CNN'])
plt.suptitle('Validation loss and accuracy for digits classification')

plt.show()

"""### Fashion MNIST


"""

# Set parameters
fashion_input_shape = next(iter(fashion_dataloader['train']))[0].shape
fashion_num_inputs = np.prod(fashion_input_shape[-2:])
fashion_num_outputs = len(np.unique(fashion_dataloader['test'].dataset.targets))

dtype = torch.float
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

print(f'Device: {device}')

# Create SNN model
fashion_snn = SNN(fashion_num_inputs, fashion_num_outputs).to(device)
fashion_snn_optimizer = torch.optim.Adam(fashion_snn.parameters(), lr=0.000085, betas=(0.9, 0.999))

print(fashion_snn)

# Train SNN model
fashion_snn_loss, fashion_snn_accuracy, fashion_snn_time = training(fashion_snn, fashion_dataloader['train'], 
                                  fashion_dataloader['val'], fashion_snn_optimizer, 
                                  criterion(), device, max_epochs, 
                                  snn_mode=True, num_steps=time_steps)

# Create CNN model
fashion_cnn = CNN(fashion_input_shape, fashion_num_outputs).to(device)
fashion_cnn_optimizer = torch.optim.Adam(fashion_cnn.parameters(), lr=0.000085, betas=(0.9, 0.999))

print(fashion_cnn)

# Train CNN model
fashion_cnn_loss, fashion_cnn_accuracy, fashion_cnn_time = training(fashion_cnn, fashion_dataloader['train'], 
                                  fashion_dataloader['val'], fashion_cnn_optimizer, 
                                  criterion(), device, max_epochs, snn_mode=False)

"""#### Analisys of the results obtained by the models"""

print("Average time per epoch for SNN:", fashion_snn_time, end=" sec")
print(classification_report(*testing(fashion_snn, fashion_dataloader['test'], device, snn_mode = True)))

print("Average time per epoch for CNN:", fashion_cnn_time, end=" sec")
print(classification_report(*testing(fashion_cnn, fashion_dataloader['test'], device, snn_mode=False)))

# Plot validation loss and accuracy for both models
plt.figure(figsize=(15, 5))

plt.subplot(121)
plt.plot(fashion_snn_loss)
plt.plot(fashion_cnn_loss)

plt.ylabel('Validation loss')
plt.xlabel('Epochs')
plt.legend(['SNN', 'CNN'])

plt.subplot(122)
plt.plot(fashion_snn_accuracy)
plt.plot(fashion_cnn_accuracy)
plt.ylabel('Validation accuracy')
plt.xlabel('Epochs')
plt.legend(['SNN', 'CNN'])
plt.suptitle('Validation loss and accuracy for fashion products classification')

plt.show()